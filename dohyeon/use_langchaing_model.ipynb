{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"9c9pRzhr1FbD"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n","!pip install -q datasets loralib sentencepiece\n","!pip -q install bitsandbytes accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fcegCDbx1XNt"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N14anFTx2_xr"},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, TextGenerationPipeline,pipeline\n","import torch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fud3xma21Zme"},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","\n","# 모델 로드\n","base_model = BartForConditionalGeneration.from_pretrained(\n","    \"junseokkim00/KoBART_Question_Generation\"\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":439,"status":"ok","timestamp":1713757152110,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"D56pYeVL3O1j","outputId":"7fa9570f-7690-4f4a-8c0b-de0aa67cb2eb"},"outputs":[{"name":"stderr","output_type":"stream","text":["The model 'BartForConditionalGeneration' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"]}],"source":["pipe = pipeline(\n","    \"text-generation\",\n","    model=base_model,\n","    tokenizer=tokenizer,\n","    max_length=512,\n","    temperature=0.7,\n","    top_p=0.95,\n","    repetition_penalty=1.15\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11127,"status":"ok","timestamp":1713757792485,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"gFZgXDsT7GOV","outputId":"573c1430-380c-4ea6-e3e0-ae380825230e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain\n","  Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML\u003e=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy\u003c3,\u003e=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n","Requirement already satisfied: aiohttp\u003c4.0.0,\u003e=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n","Requirement already satisfied: async-timeout\u003c5.0.0,\u003e=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json\u003c0.7,\u003e=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting jsonpatch\u003c2.0,\u003e=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-community\u003c0.1,\u003e=0.0.32 (from langchain)\n","  Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-core\u003c0.2.0,\u003e=0.1.42 (from langchain)\n","  Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-text-splitters\u003c0.1,\u003e=0.0.1 (from langchain)\n","  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n","Collecting langsmith\u003c0.2.0,\u003e=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003c2,\u003e=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic\u003c3,\u003e=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n","Requirement already satisfied: requests\u003c3,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity\u003c9.0.0,\u003e=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain) (1.3.1)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain) (23.2.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain) (1.4.1)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain) (6.0.5)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain) (1.9.4)\n","Collecting marshmallow\u003c4.0.0,\u003e=3.18.0 (from dataclasses-json\u003c0.7,\u003e=0.5.7-\u003elangchain)\n","  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect\u003c1,\u003e=0.4.0 (from dataclasses-json\u003c0.7,\u003e=0.5.7-\u003elangchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting jsonpointer\u003e=1.9 (from jsonpatch\u003c2.0,\u003e=1.33-\u003elangchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Collecting packaging\u003c24.0,\u003e=23.2 (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain)\n","  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson\u003c4.0.0,\u003e=3.9.14 (from langsmith\u003c0.2.0,\u003e=0.1.17-\u003elangchain)\n","  Downloading orjson-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain) (2.18.1)\n","Requirement already satisfied: typing-extensions\u003e=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain) (4.11.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangchain) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangchain) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangchain) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy\u003c3,\u003e=1.4-\u003elangchain) (3.0.3)\n","Collecting mypy-extensions\u003e=0.3.0 (from typing-inspect\u003c1,\u003e=0.4.0-\u003edataclasses-json\u003c0.7,\u003e=0.5.7-\u003elangchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.0\n","    Uninstalling packaging-24.0:\n","      Successfully uninstalled packaging-24.0\n","Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.16 langchain-community-0.0.34 langchain-core-0.1.45 langchain-text-splitters-0.0.1 langsmith-0.1.49 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 typing-inspect-0.9.0\n"]}],"source":["!pip install langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12134,"status":"ok","timestamp":1713757829394,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"bD1oepIT7NM_","outputId":"c188af69-e1f7-4fd1-a32a-94ab45358da2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain_openai\n","  Downloading langchain_openai-0.1.3-py3-none-any.whl (33 kB)\n","Requirement already satisfied: langchain-core\u003c0.2.0,\u003e=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.1.45)\n","Collecting openai\u003c2.0.0,\u003e=1.10.0 (from langchain_openai)\n","  Downloading openai-1.23.2-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tiktoken\u003c1,\u003e=0.5.2 (from langchain_openai)\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML\u003e=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (6.0.1)\n","Requirement already satisfied: jsonpatch\u003c2.0,\u003e=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (1.33)\n","Requirement already satisfied: langsmith\u003c0.2.0,\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (0.1.49)\n","Requirement already satisfied: packaging\u003c24.0,\u003e=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (23.2)\n","Requirement already satisfied: pydantic\u003c3,\u003e=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (2.7.0)\n","Requirement already satisfied: tenacity\u003c9.0.0,\u003e=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (8.2.3)\n","Requirement already satisfied: anyio\u003c5,\u003e=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (3.7.1)\n","Requirement already satisfied: distro\u003c2,\u003e=1.7.0 in /usr/lib/python3/dist-packages (from openai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (1.7.0)\n","Collecting httpx\u003c1,\u003e=0.23.0 (from openai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm\u003e4 in /usr/local/lib/python3.10/dist-packages (from openai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (4.66.2)\n","Requirement already satisfied: typing-extensions\u003c5,\u003e=4.7 in /usr/local/lib/python3.10/dist-packages (from openai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (4.11.0)\n","Requirement already satisfied: regex\u003e=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken\u003c1,\u003e=0.5.2-\u003elangchain_openai) (2023.12.25)\n","Requirement already satisfied: requests\u003e=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken\u003c1,\u003e=0.5.2-\u003elangchain_openai) (2.31.0)\n","Requirement already satisfied: idna\u003e=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.5.0-\u003eopenai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio\u003c5,\u003e=3.5.0-\u003eopenai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (1.2.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx\u003c1,\u003e=0.23.0-\u003eopenai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx\u003c1,\u003e=0.23.0-\u003eopenai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11\u003c0.15,\u003e=0.13 (from httpcore==1.*-\u003ehttpx\u003c1,\u003e=0.23.0-\u003eopenai\u003c2.0.0,\u003e=1.10.0-\u003elangchain_openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jsonpointer\u003e=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch\u003c2.0,\u003e=1.33-\u003elangchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (2.4)\n","Requirement already satisfied: orjson\u003c4.0.0,\u003e=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (3.10.1)\n","Requirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain-core\u003c0.2.0,\u003e=0.1.42-\u003elangchain_openai) (2.18.1)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.26.0-\u003etiktoken\u003c1,\u003e=0.5.2-\u003elangchain_openai) (3.3.2)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003e=2.26.0-\u003etiktoken\u003c1,\u003e=0.5.2-\u003elangchain_openai) (2.0.7)\n","Installing collected packages: h11, tiktoken, httpcore, httpx, openai, langchain_openai\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.3 openai-1.23.2 tiktoken-0.6.0\n","Requirement already satisfied: langchain_core in /usr/local/lib/python3.10/dist-packages (0.1.45)\n","Requirement already satisfied: PyYAML\u003e=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n","Requirement already satisfied: jsonpatch\u003c2.0,\u003e=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (1.33)\n","Requirement already satisfied: langsmith\u003c0.2.0,\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (0.1.49)\n","Requirement already satisfied: packaging\u003c24.0,\u003e=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (23.2)\n","Requirement already satisfied: pydantic\u003c3,\u003e=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.7.0)\n","Requirement already satisfied: tenacity\u003c9.0.0,\u003e=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.2.3)\n","Requirement already satisfied: jsonpointer\u003e=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch\u003c2.0,\u003e=1.33-\u003elangchain_core) (2.4)\n","Requirement already satisfied: orjson\u003c4.0.0,\u003e=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain_core) (3.10.1)\n","Requirement already satisfied: requests\u003c3,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from langsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain_core) (2.31.0)\n","Requirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain_core) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain_core) (2.18.1)\n","Requirement already satisfied: typing-extensions\u003e=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic\u003c3,\u003e=1-\u003elangchain_core) (4.11.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain_core) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain_core) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain_core) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2-\u003elangsmith\u003c0.2.0,\u003e=0.1.0-\u003elangchain_core) (2024.2.2)\n"]}],"source":["!pip install langchain_openai\n","!pip install langchain_core"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"elapsed":1078,"status":"ok","timestamp":1713761395497,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"X95WF4pT4t_G","outputId":"4b22f6b0-4d89-4b6a-93af-37ddf8459318"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["''"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["import json\n","import textwrap\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","\n","# 모델 로드\n","base_model = BartForConditionalGeneration.from_pretrained(\n","    \"junseokkim00/KoBART_Question_Generation\"\n",")\n","\n","# 대화 템플릿 설정\n","prompt_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You are a professional interviewer. Examine the provided context and based on the keyword in the following sentence, create a question that requires an answer not found within the sentence itself. Please format your question with the main points in bullet points and ensure the question ends with a '?'. Additionally, translate your question to Korean.\"\n","        ),\n","        (\n","            \"user\",\n","            \"Please make question according to the following request.\"\n","            \"\\nREQUEST:\\n\"\n","            \"1. make question the main points in bullet points in Korean.\"\n","            \"2. The last part of the context is '?' It must be this.\"\n","            \"3.  Look at the following sentence and ask a question based on the keyword in the next sentence but question requires an answer not found within the sentence itself\"\n","            \"4. Look at the following sentence and ask a question based on the keyword in the next sentence which  answer cannot be found within the sentence\"\n","            \"5. if question is not Korean, translate to Korean\"\n","            \"6. Do NOT translate Korean to English\"\n","            \"7. print 3 questions and end.\"\n","            \"\\n\\nCONTEXT: {context}\\n\\nSUMMARY:\",\n","        ),\n","    ]\n",")\n","\n","def get_prompt(human_prompt):\n","    # prompt_template=f\"{human_prompt}\"\n","    prompt_template=f\"{human_prompt} \\n### Response:\"\n","    return prompt_template\n","\n","\n","def remove_human_text(text):\n","    return text.split('### Human:', 1)[0]\n","\n","def parse_text_after_input(data, input_string):\n","    for item in data:\n","        text = item['generated_text']\n","        input_string_index = text.find(input_string)\n","        if input_string_index != -1:\n","            output_text = text[input_string_index+len(input_string):].strip()\n","            output_text = parse_text(output_text)\n","            wrapped_text = textwrap.fill(output_text, width=100)\n","            print(wrapped_text)\n","def parse_text(data):\n","    for item in data:\n","        text = item['generated_text']\n","        assistant_text_index = text.find('### Response:')\n","        if assistant_text_index != -1:\n","            assistant_text = text[assistant_text_index+len('### Response:'):].strip()\n","            assistant_text = remove_human_text(assistant_text)\n","            wrapped_text = textwrap.fill(assistant_text, width=100)\n","            print(wrapped_text)\n","        return assistant_text\n","\n","\n","# 예제 문맥\n","context =\"일단은 단점이라고 생각하는 부분은 없고요. 딱히 들어본 기억이 없어요. 너는 이런 게 문제야 혹은 또는 이런 게 부족해 라고 하는 말을 들은 적도 없고 그런 피드백을 받은 적도 없고 근까 학교에서 뭔가 과제를 한다거나 그 전에 직장에서 제가 무슨 업무를 수행할 때 그거에 대한 피드백에서 단점 제가 성 어떤 제가 가지고 있는 단점 같은 거를 들은 기억이 없어요. 그래서 저는 사실 저의 단점은 제 안에서는 어 무수히 많은 것들이 있을 거라고 생각을 하는데 타인이 봤을 때는 굳이 없었던 건 아닐까 라고 생각합니다. 그리고 장점은 항상 좀 꼿꼿하다는 말을 많이 들었던 것 같아요. 항상 들었던 말이 너는 굉장히 바르고 자세가 흐트러지지 않는다 그리고 항상 확신에 차 있다 이런 말들을 들었던 것 같아요. 근데 얼추 제가 삶을 대하는 태도랑 비슷한 것 같긴 하더라구요.,저는 개인적인 단점은 많지만 타인이 봤을 때는 굳이 없다고 생각합니다. 장점은 바르고 자세가 흐트러지지 않으며 확신에 차있다 라는 말을 들었던 것 같은데 제가 삶을 대하는 태도랑 비슷한 것 같습니다.\"\n","formatted_prompt = get_prompt(prompt_template.format(context=context))\n","\n","# 예제 데이터 (시스템에서 생성된 응답을 가정)\n","generated_data = [{'generated_text': formatted_prompt}]\n","parse_text(generated_data)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c30VTOIRCDn9"},"source":["# 중간 저장 : 모델을 가지고 HUGGGING FACE 모델을 가지고 LANGCHAIN 기법 사용하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1834,"status":"ok","timestamp":1713759512710,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"7jR4ty2J7auq","outputId":"cb26f61d-b194-4134-d63e-b6818b275556"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["question: 저의 단점은 제 안에서는 무수히 많은 것들이 있을 거라고 생각을 하는데 타인이 봤을 때는 굳이 없었던 건 아닐까?\n","question: 저의 단점은 제 안에서는 무수히 많은 것들이 있을 거라고 생각을 하는데 타인이 봤을 때 굳이 없었던 건 아닐까?\n","question: 저의 단점은 제 안에서는 무수히 많은 것들이 있을 거라고 생각을 하는데 타인이 봤을 때는 굳이 없었던 건 아닐까요?\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","model = BartForConditionalGeneration.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","\n","def generate_questions(context):\n","    # 입력 문맥을 토크나이징\n","    inputs = tokenizer.encode(\"question: \" + context, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","    # 질문 생성\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=64,\n","        num_beams=5,  # num_beams를 5로 설정하여 더 많은 가능성을 탐색\n","        num_return_sequences=3,  # 여러 개의 질문 반환\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","\n","    # 생성된 시퀀스를 텍스트로 디코딩\n","    questions = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n","    return questions\n","\n","# 예제 문맥\n","context = \"일단은 단점이라고 생각하는 부분은 없고요. 딱히 들어본 기억이 없어요. 너는 이런 게 문제야 혹은 또는 이런 게 부족해 라고 하는 말을 들은 적도 없고 그런 피드백을 받은 적도 없고 근까 학교에서 뭔가 과제를 한다거나 그 전에 직장에서 제가 무슨 업무를 수행할 때 그거에 대한 피드백에서 단점 제가 성 어떤 제가 가지고 있는 단점 같은 거를 들은 기억이 없어요. 그래서 저는 사실 저의 단점은 제 안에서는 어 무수히 많은 것들이 있을 거라고 생각을 하는데 타인이 봤을 때는 굳이 없었던 건 아닐까 라고 생각합니다.\"\n","\n","# 질문 생성 및 출력\n","questions = generate_questions(context)\n","for question in questions:\n","    print(question)\n"]},{"cell_type":"markdown","metadata":{"id":"iQIxbulECMye"},"source":["# 여기부턴 ReACT 기법 사용하기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1907,"status":"ok","timestamp":1713760811097,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"-ujaOJPYCd5r","outputId":"d48170a4-0dc1-4909-c33a-6c4373f548ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Sehong/kobart-QuestionGeneration\")\n","model = BartForConditionalGeneration.from_pretrained(\"Sehong/kobart-QuestionGeneration\")\n","\n","class DocstoreExplorer:\n","    def __init__(self, source):\n","        self.source = source\n","\n","    def search(self, query):\n","        print(f\"Searching for: {query}\")\n","        return f\"Search results for {query}\"\n","\n","    def lookup(self, keyword):\n","        print(f\"Looking up: {keyword}\")\n","        return f\"Lookup results for {keyword}\"\n","\n","def generate_professional_questions(context, model, tokenizer):\n","    inputs = tokenizer.encode(\"question: \" + context, return_tensors=\"pt\", max_length=512, truncation=True)\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=64,\n","        num_beams=5,\n","        num_return_sequences=3,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","    questions = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n","\n","    # 면접관 스타일로 질문 개선 및 대명사 변경\n","    professional_questions = []\n","    for question in questions:\n","        # 문장 중복 및 자연스러움 개선\n","        refined_question = question.replace('당신의 자신에게도', '당신에게')\n","        refined_question = refined_question.rstrip(' 것은?').replace('는?', '는 것에 대해 어떻게 생각하십니까?').replace('있는', '있는지 설명해 주십시오.')\n","        professional_questions.append(f\"면접 상황에서: {refined_question}\")\n","\n","    return professional_questions\n","\n","def refine_questions(questions, docstore):\n","    refined_questions = []\n","    for question in questions:\n","        lookup_result = docstore.lookup(question)  # 조회하여 얻은 정보를 질문 개선에 사용\n","        refined_question = f\"Refined Question based on lookup: {lookup_result}\"\n","        refined_questions.append(refined_question)\n","    return refined_questions\n","\n","docstore = DocstoreExplorer(\"Wikipedia\")\n","# 예제 문맥에 대해 질문 생성 및 개선\n","context = \"다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는 것이 단점이며, 장점으로는 제가 더 성장하게 해주고 좀 더 발전시켜 주는 부분이 있지 않을까 싶습니다.\"\n","professional_questions = generate_professional_questions(context, model, tokenizer)\n","\n","for question in professional_questions:\n","    print(question)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1730,"status":"ok","timestamp":1713760784355,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"5CvQgOR0DXZQ","outputId":"142fafa3-f706-485d-9581-1a1ae4287fc0"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["- question 발전이 주요 이슈가 된 떄는??\n","- question 발전이 주요 이슈가 된  떄는??\n","- question 발전이 주요 이슈가 된  떄는  언제인가??\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","model = BartForConditionalGeneration.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","\n","def generate_questions_based_on_prompt(context, model, tokenizer):\n","    # 문맥 분석 및 키워드 추출 (예시로 단순화)\n","    keywords = context.split()[-3:]  # 마지막 세 단어를 키워드로 가정\n","    keyword = \" \".join(keywords)  # 키워드를 문장으로 결합\n","\n","    # 질문 생성\n","    prompt_text = f\"question: {keyword}\"\n","    inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=64,\n","        num_beams=5,\n","        num_return_sequences=3,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","\n","    questions = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n","\n","    # 질문 한국어로 번역 및 형식화 (여기서는 직접 번역하고 형식을 맞추는 것으로 가정)\n","    formatted_questions = [f\"- {question}?\" for question in questions]  # 각 질문을 총알 목록 형태로 변환\n","    return formatted_questions\n","\n","# 예제 문맥\n","context = \"최근에 기술의 발전이 사회에 미치는 영향에 대해 많은 토론이 있었습니다. 이 중에서도 인공지능 기술의 발전이 주요 이슈입니다.\"\n","\n","# 질문 생성\n","questions = generate_questions_based_on_prompt(context, model, tokenizer)\n","\n","# 질문 출력\n","for question in questions:\n","    print(question)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WH1B1llGHpFo"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"GVpVQLkCHpsI"},"source":["프롬프트 엔지니어링을 위한 접근 방식:\n","프롬프트의 지속적 사용: 제공된 프롬프트 구조를 유지하면서 각 단계에서 입력된 데이터를 기반으로 질문을 생성하고 이를 다음 단계의 입력으로 사용합니다.\n","프롬프트를 활용한 데이터 처리: 입력된 문맥을 분석하고, 이를 바탕으로 추가적인 질문이나 키워드를 생성하여 다음 프롬프트의 입력으로 활용합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6628,"status":"ok","timestamp":1713767442984,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"zdcDA5Vnf7QI","outputId":"0d8f22ec-6365-48f7-e0fe-5db4532d66ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1\u003e=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml\u003e=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy\u003e=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1\u003e=0.7.0-\u003ekonlpy) (23.2)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}],"source":["!pip install konlpy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5462,"status":"ok","timestamp":1713767562208,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"QWzclkA5GkYv","outputId":"e8d668e2-7f52-49f6-a843-5dbeea398730"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["question는 다른에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question은 사람에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question 단점에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question는 자신에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question 자책에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","경향에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question 단점에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question: 장점에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question 발전에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n","question 부분에 대해 사회적으로 어떤 영향을 미칠 수 있습니까??\n"]}],"source":["from konlpy.tag import Okt\n","from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# KoNLPy의 Okt 객체 생성\n","okt = Okt()\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","model = BartForConditionalGeneration.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","\n","def extract_important_keywords(text):\n","    # 문장 분석을 통해 명사 추출\n","    nouns = okt.nouns(text)\n","    # 중요도 분석을 통해 키워드 선정 (예시로 단순화)\n","    important_nouns = [noun for noun in nouns if len(noun) \u003e 1]  # 두 글자 이상의 명사만 선택\n","    return important_nouns\n","\n","def generate_questions(context, model, tokenizer):\n","    keywords = extract_important_keywords(context)\n","    questions = []\n","\n","    for keyword in keywords:\n","        # 키워드를 활용한 깊이 있는 질문 생성\n","        prompt_text = f\"question: {keyword}에 대해 사회적으로 어떤 영향을 미칠 수 있습니까?\"\n","        inputs = tokenizer.encode(prompt_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","        outputs = model.generate(\n","            input_ids=inputs,\n","            max_length=100,\n","            num_beams=5,\n","            num_return_sequences=1,\n","            no_repeat_ngram_size=2,\n","            early_stopping=True\n","        )\n","        question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        questions.append(question + \"?\")\n","\n","    return questions\n","\n","# 예제 문맥\n","context = \"다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는 것이 단점이며, 장점으로는 제가 더 성장하게 해주고 좀 더 발전시켜 주는 부분이 있지 않을까 싶습니다.\"\n","\n","# 질문 생성 및 출력\n","questions = generate_questions(context, model, tokenizer)\n","for question in questions:\n","    print(question)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1713767879522,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"gfGid-u4IDFf","outputId":"df018560-e6f1-4135-b94d-1fe3276838a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["The capital city of England is London.\n"]}],"source":["import json\n","import textwrap\n","\n","# 사용자가 제공한 프롬프트를 기반으로 출력 포맷을 설정하는 함수\n","def get_prompt(human_prompt):\n","    prompt_template = f\"{human_prompt} \\n### Response:\"\n","    return prompt_template\n","\n","# \"### Human:\" 텍스트를 제거하는 함수\n","def remove_human_text(text):\n","    return text.split('### Human:', 1)[0]\n","\n","# 입력된 텍스트 이후의 데이터를 파싱하는 함수\n","def parse_text_after_input(data, input_string):\n","    for item in data:\n","        text = item['generated_text']\n","        input_string_index = text.find(input_string)\n","        if input_string_index != -1:\n","            output_text = text[input_string_index+len(input_string):].strip()\n","            output_text = parse_text(output_text)\n","            wrapped_text = textwrap.fill(output_text, width=100)\n","            print(wrapped_text)\n","\n","# 데이터를 파싱하여 응답 부분만 추출하는 함수\n","def parse_text(data):\n","    for item in data:\n","        text = item['generated_text']\n","        assistant_text_index = text.find('### Response:')\n","        if assistant_text_index != -1:\n","            assistant_text = text[assistant_text_index+len('### Response:'):].strip()\n","            assistant_text = remove_human_text(assistant_text)\n","            wrapped_text = textwrap.fill(assistant_text, width=100)\n","            print(wrapped_text)\n","\n","# 테스트 데이터와 함수 호출 예제\n","data = [{'generated_text': '### Human: What is the capital of England? \\n### Response: The capital city of England is London.'}]\n","parse_text(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VpmLMg3UIDDA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4069,"status":"ok","timestamp":1713761198758,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"DYc4XeZ_HeH9","outputId":"35339940-9790-44b7-f9c2-0889a16e4ce3"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자신의 자신에게도 자책하는 경향이 있는 것은??\n","question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는 것은??\n","question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자기에게도 자책하는 경향이 있는 것은??\n","다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는 것은??\n","다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는 것은 누구인가??\n","다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는것이 단점인 것은??\n","question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는 것은??\n","다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는것이 단점인 것은??\n","다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 자책하는 경향이 있는 것은??\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","model = BartForConditionalGeneration.from_pretrained(\"junseokkim00/KoBART_Question_Generation\")\n","\n","def apply_prompt_engineering(context, model, tokenizer):\n","    # 초기 문맥에 대한 질문 생성\n","    initial_questions = generate_questions_based_on_context(context, model, tokenizer)\n","\n","    # 생성된 질문을 바탕으로 추가 질문 생성\n","    all_questions = []\n","    for question in initial_questions:\n","        further_questions = generate_questions_based_on_context(question, model, tokenizer)\n","        all_questions.extend(further_questions)\n","\n","    return all_questions\n","\n","def generate_questions_based_on_context(context, model, tokenizer):\n","    inputs = tokenizer.encode(\"question: \" + context, return_tensors=\"pt\", max_length=512, truncation=True)\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=64,\n","        num_beams=5,\n","        num_return_sequences=3,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","\n","    return [tokenizer.decode(generated_sequence, skip_special_tokens=True) + \"?\" for generated_sequence in output_sequences]\n","\n","# 예제 문맥\n","context = \"다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는 것이 단점이며, 장점으로는 제가 더 성장하게 해주고 좀 더 발전시켜 주는 부분이 있지 않을까 싶습니다.\"\n","\n","# 프롬프트 엔지니어링 적용\n","questions = apply_prompt_engineering(context, model, tokenizer)\n","\n","# 결과 출력\n","for question in questions:\n","    print(question)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3STun4JRfJS"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"CeWtEPHLS1i6"},"source":["## ㅁㄴㄴ\n","\\"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1952,"status":"ok","timestamp":1713764024609,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"6GDskQ7JS3JJ","outputId":"8f0ea25e-31a5-4d1a-aa15-a1cba88968d2"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Looking up: 면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Looking up: 면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Looking up: 면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on lookup: Lookup results for 면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on lookup: Lookup results for 면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on lookup: Lookup results for 면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Sehong/kobart-QuestionGeneration\")\n","model = BartForConditionalGeneration.from_pretrained(\"Sehong/kobart-QuestionGeneration\")\n","\n","def generate_professional_questions(context, model, tokenizer):\n","    inputs = tokenizer.encode(\"question: \" + context, return_tensors=\"pt\", max_length=512, truncation=True)\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=64,\n","        num_beams=5,\n","        num_return_sequences=3,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","    questions = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n","\n","    # 면접관 스타일로 질문 개선 및 대명사 변경\n","    professional_questions = []\n","    for question in questions:\n","        # 문장 중복 및 자연스러움 개선\n","        refined_question = question.replace('당신의 자신에게도', '당신에게')\n","        refined_question = refined_question.rstrip(' 것은?').replace('는?', '는 것에 대해 어떻게 생각하십니까?').replace('있는', '있는지 설명해 주십시오.')\n","        professional_questions.append(f\"면접 상황에서: {refined_question}\")\n","\n","    return professional_questions\n","\n","# 예제 문맥에 대해 질문 생성\n","context = \"다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는 것이 단점이며, 장점으로는 제가 더 성장하게 해주고 좀 더 발전시켜 주는 부분이 있지 않을까 싶습니다.\"\n","professional_questions = generate_professional_questions(context, model, tokenizer)\n","\n","for question in professional_questions:\n","    print(question)\n","\n","class DocstoreExplorer:\n","    def __init__(self, source):\n","        self.source = source\n","\n","    def lookup(self, keyword):\n","        print(f\"Looking up: {keyword}\")\n","        return f\"Lookup results for {keyword}\"\n","\n","def refine_questions(questions, docstore):\n","    refined_questions = []\n","    for question in questions:\n","        lookup_result = docstore.lookup(question)  # 조회하여 얻은 정보를 질문 개선에 사용\n","        refined_question = f\"Refined Question based on lookup: {lookup_result}\"\n","        refined_questions.append(refined_question)\n","    return refined_questions\n","\n","docstore = DocstoreExplorer(\"Wikipedia\")\n","# 예제 문맥에 대해 질문 개선\n","refined_questions = refine_questions(professional_questions, docstore)\n","\n","for question in refined_questions:\n","    print(question)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1474,"status":"ok","timestamp":1713764234606,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"da4ibF7bS7aP","outputId":"16fe07c1-c789-478b-df27-6cc898778138"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]},{"name":"stdout","output_type":"stream","text":["면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Searching: 면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Looking up: 면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Searching: 면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Looking up: 면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Searching: 면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Looking up: 면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on search: Search results for 면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on lookup: Lookup results for 면접 상황에서: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on search: Search results for 면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on lookup: Lookup results for 면접 상황에서: question: 다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on search: Search results for 면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n","Refined Question based on lookup: Lookup results for 면접 상황에서: 다른사람 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는지 설명해 주십시오.\n"]}],"source":["from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n","\n","# 토크나이저와 모델 로드\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Sehong/kobart-QuestionGeneration\")\n","model = BartForConditionalGeneration.from_pretrained(\"Sehong/kobart-QuestionGeneration\")\n","\n","def generate_professional_questions(context, model, tokenizer):\n","    inputs = tokenizer.encode(\"question: \" + context, return_tensors=\"pt\", max_length=512, truncation=True)\n","    output_sequences = model.generate(\n","        input_ids=inputs,\n","        max_length=64,\n","        num_beams=5,\n","        num_return_sequences=3,\n","        no_repeat_ngram_size=2,\n","        early_stopping=True\n","    )\n","    questions = [tokenizer.decode(generated_sequence, skip_special_tokens=True) for generated_sequence in output_sequences]\n","\n","    # 면접관 스타일로 질문 개선 및 대명사 변경\n","    professional_questions = []\n","    for question in questions:\n","        # 문장 중복 및 자연스러움 개선\n","        refined_question = question.replace('당신의 자신에게도', '당신에게')\n","        refined_question = refined_question.rstrip(' 것은?').replace('는?', '는 것에 대해 어떻게 생각하십니까?').replace('있는', '있는지 설명해 주십시오.')\n","        professional_questions.append(f\"면접 상황에서: {refined_question}\")\n","\n","    return professional_questions\n","\n","# 예제 문맥에 대해 질문 생성\n","context = \"다른 사람이 봤을 때 제 단점은 예민하고 섬세할 수 있지만 제 자신에게도 자책하는 경향이 있는 것이 단점이며, 장점으로는 제가 더 성장하게 해주고 좀 더 발전시켜 주는 부분이 있지 않을까 싶습니다.\"\n","professional_questions = generate_professional_questions(context, model, tokenizer)\n","\n","for question in professional_questions:\n","    print(question)\n","class DocstoreExplorer:\n","    def __init__(self, source):\n","        self.source = source\n","\n","    def search(self, keyword):\n","        print(f\"Searching: {keyword}\")\n","        return f\"Search results for {keyword}\"\n","\n","    def lookup(self, keyword):\n","        print(f\"Looking up: {keyword}\")\n","        return f\"Lookup results for {keyword}\"\n","\n","def refine_questions(questions, docstore):\n","    refined_questions = []\n","    for question in questions:\n","        search_result = docstore.search(question)  # 검색하여 얻은 정보를 질문 개선에 사용\n","        lookup_result = docstore.lookup(question)  # 조회하여 얻은 정보를 질문 개선에 사용\n","        refined_question = f\"Refined Question based on search: {search_result}\\nRefined Question based on lookup: {lookup_result}\"\n","        refined_questions.append(refined_question)\n","    return refined_questions\n","\n","docstore = DocstoreExplorer(\"Wikipedia\")\n","# 예제 문맥에 대해 질문 개선\n","refined_questions = refine_questions(professional_questions, docstore)\n","\n","for question in refined_questions:\n","    print(question)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MCoeEclQTu1r"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Yf-D_MTpZBSA"},"source":["# \u003c ㅁㄴㅇㅁㄴㅇ\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713766436956,"user":{"displayName":"김도현 (김도현_지역청년)","userId":"10173226597444429317"},"user_tz":-540},"id":"NR0HDbc5Y-IG","outputId":"a0e8c470-06a1-4850-8c5d-dc8138856783"},"outputs":[{"name":"stdout","output_type":"stream","text":["• 키워드1와(과) 관련된 사항 중에서, 문서에 명시되지 않은 정보는 무엇인가요?\n","• 키워드2와(과) 관련된 사항 중에서, 문서에 명시되지 않은 정보는 무엇인가요?\n","• 키워드3와(과) 관련된 사항 중에서, 문서에 명시되지 않은 정보는 무엇인가요?\n"]}],"source":["class ChatPromptTemplate:\n","    def __init__(self, context):\n","        self.context = context\n","        self.questions = []\n","\n","    def generate_questions_based_on_keywords(self, keywords):\n","        # 이 메서드는 키워드를 기반으로 질문을 생성합니다.\n","        # 실제 구현에서는 모델을 활용하여 컨텍스트에 기반한 질문을 생성하는 로직이 필요합니다.\n","        for keyword in keywords:\n","            question = f\"• {keyword}와(과) 관련된 사항 중에서, 문서에 명시되지 않은 정보는 무엇인가요?\"\n","            self.questions.append(question)\n","\n","    def translate_questions_to_korean(self):\n","        # 이 메서드는 질문들을 한국어로 번역합니다.\n","        # 실제 구현에서는 번역 모델을 활용하여 질문을 번역하는 로직이 필요합니다.\n","        # 여기서는 번역 과정을 생략하고 질문을 그대로 반환하도록 하겠습니다.\n","        translated_questions = [question for question in self.questions]\n","        return translated_questions\n","\n","    def print_questions(self):\n","        translated_questions = self.translate_questions_to_korean()\n","        for question in translated_questions:\n","            print(question)\n","\n","# 사용 예시\n","if __name__ == \"__main__\":\n","    context = \"여기에 분석할 컨텍스트를 입력하세요.\"\n","    keywords = [\"키워드1\", \"키워드2\", \"키워드3\"]\n","    chat_prompt_template = ChatPromptTemplate(context)\n","    chat_prompt_template.generate_questions_based_on_keywords(keywords)\n","    chat_prompt_template.print_questions()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEiNCq8NZLMV"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP9LserBRUUVFLxHIKxle8x","gpuType":"A100","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}